{"cells": [{"cell_type": "code", "execution_count": 92, "metadata": {}, "outputs": [], "source": "import pyspark\nfrom pyspark import SparkConf, SparkContext\nimport pandas as pd\nfrom pyspark.sql import SparkSession, SQLContext\nfrom datetime import datetime\nimport json\nfrom pyspark.sql.types import *\nfrom datetime import timedelta\nfrom operator import add"}, {"cell_type": "code", "execution_count": 134, "metadata": {}, "outputs": [], "source": "import nltk\nfrom nltk.corpus import stopwords\nimport re as re\nfrom pyspark.ml.feature import CountVectorizer , IDF\nfrom pyspark.ml.linalg import Vector, Vectors\nfrom pyspark.ml.clustering import LDA, LDAModel"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "# functions\ndef find_keywords(row):\n    k1 = row.select('keywords').rdd\n    k1 = k1.map(lambda x: x.keywords.strip('[').strip(']')).filter(lambda x: len(x) > 0).filter(lambda x: x != 'nan')\n    k1 = k1.flatMap(lambda x: x.split(','))\n    k1 = k1.map(lambda x : (x.strip(' ').strip(\"'\").lower(),1))\n    kk1 = k1.reduceByKey(add).sortBy(lambda x: x[1] ,ascending =False)\n    return kk1.collect()\n\ndef find_journal(row): \n    j1 = row.select('journal').rdd\n    j1 = j1.map(lambda x: x.journal.strip('[').strip(']')).filter(lambda x: len(x) > 0).filter(lambda x: x != 'nan')\n    j1 = j1.map(lambda x : (x,1))\n    j1 = j1.reduceByKey(add).sortBy(lambda x: x[1] ,ascending =False)\n    return j1.collect()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# transform csv file to json file\n\npd_cancer = pd.read_csv('cancer_total.csv')\npd_cancer = pd_cancer.drop(list(pd_cancer.columns[:2]), axis=1)\n\ninfo_dict = {'pubmed_id', 'authors', 'keywords','abstract','publication_date','title','results','doi', 'journal'}\nindex_test = 0\nfile_name =0\n\ntotal= []\n\nfor i in range(len(pd_cancer['pubmed_id'])):\n    test_dict = {}\n   \n    for key in list(info_dict):\n        test_dict[key] = str(pd_cancer[key][i])\n     \n    total.append(test_dict)\n    index_test += 1 \n    if index_test == 151500:\n        with open(f'cjson/c_{file_name}.json', 'w') as f:\n            json.dump(total, f)\n        print(index_test)\n        file_name +=1\n        \n        index_test = 0\n        total= []    "}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "spark = SparkSession.builder.appName(\"t_cancer\").getOrCreate()"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": "schema = StructType([StructField(\"pubmed_id\", StringType(), True), StructField(\"authors\", StringType(), True), StructField(\"keywords\", StringType(), True),\n                    StructField(\"abstract\", StringType(), True),StructField(\"publication_date\", DateType(), True),StructField(\"title\", StringType(), True),\n                    StructField(\"results\", StringType(), True),StructField(\"doi\", StringType(), True),StructField(\"journal\", StringType(), True)])\n\ntotal_cancer = spark.read.json(f'gs://project_rong/cancer/cjson/c_0.json', schema=schema)\nfor i in range(1,10): \n    x = spark.read.json(f'gs://project_rong/cancer/cjson/c_{i}.json', schema=schema)\n    total_cancer = total_cancer.union(x)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "time_dict = {}\n\nfor i in range(2009, 2019):\n    time_period = total_cancer.filter(total_cancer['publication_date'] >= datetime(i,1,1)).filter(total_cancer['publication_date'] <= datetime(i+1,1,1))\n    time_dict[i] = {}\n    time_dict[i]['total_articles'] = time_period.count()\n    time_dict[i]['keywords'] = find_keywords(time_period)[:20]\n    time_dict[i]['journal'] = find_journal(time_period)[:20]\n\n    \nwith open('total_cancer.json', 'w') as f:\n    json.dump(time_dict, f)\n    \n! gsutil cp total_cancer.json gs://project_rong/cancer/total_cancer.json"}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [], "source": " # LDA model \nsp1 = total_cancer.select('abstract').rdd\nsp2 = sp1.filter(lambda x:  x.abstract != 'nan')\ntokens = sp2.map( lambda x: x.abstract.strip().lower()).map( lambda x: x.replace('\\n' , ''))\\\n.map( lambda x: x.replace('\\t' , '')).map( lambda abstract: re.split(\" \", abstract))\\\n.map( lambda word: [x for x in word if x.isalpha()])\\\n.map(lambda word: [x for x in word if len(x) >= 3])\\\n.map( lambda word: [x for x in word if x not in StopWords]).zipWithIndex()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sqlContext = SQLContext(spark)\ndf_txts = sqlContext.createDataFrame(tokens, [\"list_of_words\",'index'])\n\ncv = CountVectorizer(inputCol=\"list_of_words\", outputCol=\"raw_features\", vocabSize=5000, minDF=10.0)\ncvmodel = cv.fit(df_txts)\nresult_cv = cvmodel.transform(df_txts)\n\n# IDF\nidf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\nidfModel = idf.fit(result_cv)\nresult_tfidf = idfModel.transform(result_cv) \n\n# choosenumber of topics\nnum_topics = 10\nmax_iterations = 100\nlda = LDA(k=num_topics, maxIter=max_iterations)\nlda_model = lda.fit(result_tfidf)\n\n\ntopics = lda_model.describeTopics(10)\nax = lda_model.describeTopics(wordNumbers).rdd\ntermindex = ax.map(lambda x: x.termIndices)\nterm_weight = ax.map(lambda x: x.termWeights)"}, {"cell_type": "code", "execution_count": 114, "metadata": {}, "outputs": [], "source": "# words in topics\nresults = []\nfor each_row in termindex.collect():\n    row_word = []\n    for item in each_row: \n        word = cvmodel.vocabulary[item]\n        row_word.append(word)\n    results.append(row_word)\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "test_result = {'test': results, 'weight':term_weight.collect()}\n\nwith open ('lda_model_1211.json', 'w') as f:\n       json.dump(test_result , f)\n        \n! gsutil cp lda_model_1211.json gs://project_rong/lda_model_1211.json"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# result_tfidf[['index','features']].show()"}, {"cell_type": "code", "execution_count": 253, "metadata": {}, "outputs": [], "source": "# result_tfidf[['list_of_words', 'features']].show()\n# result_cv.show()"}, {"cell_type": "code", "execution_count": 130, "metadata": {}, "outputs": [{"data": {"text/plain": "[[0,\n  SparseVector(5000, {2: 1.1589, 5: 1.6701, 22: 1.7419, 26: 1.8097, 33: 2.154, 34: 2.1329, 60: 4.3026, 63: 4.7983, 74: 2.2263, 91: 2.794, 101: 2.2473, 112: 2.5541, 155: 2.8054, 163: 2.8796, 168: 2.9822, 174: 2.6956, 179: 2.7365, 181: 2.6945, 187: 2.6608, 193: 2.8538, 208: 2.9898, 224: 5.5805, 287: 3.2008, 316: 3.0337, 328: 3.2714, 342: 3.3235, 347: 3.1839, 374: 6.5963, 405: 3.2816, 448: 3.3718, 518: 3.5949, 561: 3.7863, 588: 3.6034, 605: 3.6468, 617: 4.0361, 634: 3.8244, 682: 4.2228, 714: 3.8458, 826: 3.9851, 916: 3.9727, 984: 8.6447, 1069: 9.2831, 1507: 4.5646, 1555: 4.7337, 1824: 4.8351, 1937: 4.7603, 2902: 5.359, 3084: 5.8063, 3094: 5.6197, 3198: 5.4625, 3652: 5.8063})]]"}, "execution_count": 130, "metadata": {}, "output_type": "execute_result"}], "source": "result_tfidf[['index','features']].rdd.map(list).take(1)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "num_topics = 10\nmax_iterations = 100\nlda = LDA(k=num_topics, maxIter=max_iterations)\nlda_model = lda.fit(result_tfidf)\n# lda_model = LDA.fit(result_tfidf[['index','features']].rdd.map(list), k=num_topics, maxIterations=max_iterations)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# lda.save('s1lda')\ntopics = lda_model.describeTopics(10)\ntopics.show(truncate=False)"}, {"cell_type": "code", "execution_count": 240, "metadata": {}, "outputs": [], "source": "wordNumbers = 10"}, {"cell_type": "code", "execution_count": 161, "metadata": {}, "outputs": [], "source": "sc = spark.sparkContext"}, {"cell_type": "code", "execution_count": 250, "metadata": {}, "outputs": [{"data": {"text/plain": "[[0.01086313526456371,\n  0.008521772761877369,\n  0.007667098108510024,\n  0.006679464117423838,\n  0.006635663372845226,\n  0.006210634351912754,\n  0.005956553463990851,\n  0.005814040440574243,\n  0.005508618321781763,\n  0.004705677390302103],\n [0.010515908222162513,\n  0.010016743546634016,\n  0.009979219991289982,\n  0.005923364473632306,\n  0.005526009465707872,\n  0.005341963195986004,\n  0.005298643375031896,\n  0.004807256895743788,\n  0.0046855985148840755,\n  0.004351431250237673],\n [0.006533760887340074,\n  0.0061429398921155004,\n  0.005975414080958479,\n  0.005840504657413391,\n  0.004951230894701462,\n  0.004438720889458821,\n  0.0037564882544591223,\n  0.003704854541068814,\n  0.0034352227537835325,\n  0.0034111691233958573],\n [0.012678995296530032,\n  0.009290218645934165,\n  0.006864402696410305,\n  0.0064190846937411975,\n  0.004921527657685131,\n  0.004592252303495899,\n  0.004579245104193475,\n  0.0044595838394146025,\n  0.004443179476983699,\n  0.004194450304107405],\n [0.012674354853201281,\n  0.00962132297913113,\n  0.007626473871060479,\n  0.00669146562224667,\n  0.006430087363082345,\n  0.006067178055855615,\n  0.006035063395727258,\n  0.006004755518010544,\n  0.005556834196415286,\n  0.005290196177029527],\n [0.006259963196684255,\n  0.005820362235667698,\n  0.00575010620212754,\n  0.005548648428751806,\n  0.00455741651869189,\n  0.004528009905269358,\n  0.00450356335353336,\n  0.004473758839142773,\n  0.004351208787126692,\n  0.0041598957966990395],\n [0.008046820961994397,\n  0.00749048363587337,\n  0.005477777823317542,\n  0.004758289571250264,\n  0.004678077055008862,\n  0.004286947347525674,\n  0.004187704168006017,\n  0.00410550173015749,\n  0.0038082225507454796,\n  0.0037073929081254875],\n [0.008560812100275495,\n  0.0075800978091295,\n  0.006695070437894358,\n  0.006096635483542672,\n  0.006045033916128099,\n  0.005643737788449801,\n  0.005634990427884302,\n  0.005023316643889613,\n  0.004581622968596262,\n  0.004463697368472454],\n [0.01292898704772256,\n  0.009343124313279732,\n  0.009254091068236334,\n  0.008003891089239304,\n  0.006508734962737604,\n  0.005997486103403452,\n  0.005575095575795177,\n  0.0053208451332334835,\n  0.005162081060510297,\n  0.005134947512462893],\n [0.009888361841722336,\n  0.007131268554145335,\n  0.006304951313066423,\n  0.006141496493724406,\n  0.005946846336584809,\n  0.005837789851283079,\n  0.005459720175739287,\n  0.005086432706059539,\n  0.0045517149094379174,\n  0.004528115990369645]]"}, "execution_count": 250, "metadata": {}, "output_type": "execute_result"}], "source": "ax = lda_model.describeTopics(wordNumbers).rdd\nx12 = ax.map(lambda x: x.termIndices)\nterm_weight = ax.map(lambda x: x.termWeights)\n\nx12.collect()\nterm_weight.collect()\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 188, "metadata": {}, "outputs": [], "source": "topicIndices = lda_model.describeTopics(wordNumbers).rdd"}, {"cell_type": "code", "execution_count": 242, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[['dose', 'brain', 'radiation', 'imaging', 'mm', 'images', 'glioma', 'volume', 'image', 'plasma'], ['dna', 'hpv', 'cells', 'cell', 'leukemia', 'damage', 'human', 'repair', 'infection', 'viral'], ['health', 'cancer', 'risk', 'women', 'screening', 'mortality', 'use', 'among', 'studies', 'breast'], ['expression', 'cell', 'cells', 'protein', 'signaling', 'growth', 'apoptosis', 'gene', 'activation', 'role'], ['patients', 'survival', 'lymph', 'stage', 'p', 'surgery', 'prognostic', 'node', 'recurrence', 'group'], ['case', 'diagnosis', 'lesions', 'liver', 'patients', 'patient', 'cases', 'bone', 'malignant', 'rare'], ['cells', 'tumor', 'melanoma', 'mice', 'immune', 'molecular', 'stem', 'endothelial', 'cell', 'growth'], ['breast', 'risk', 'cervical', 'samples', 'cases', 'genetic', 'association', 'women', 'sensitivity', 'mutation'], ['patients', 'thyroid', 'chemotherapy', 'response', 'treatment', 'median', 'toxicity', 'bladder', 'dose', 'survival'], ['prostate', 'quality', 'patients', 'life', 'care', 'pain', 'intervention', 'children', 'endometrial', 'scores']]\n"}], "source": "results = []\nfor each_row in x12.collect():\n    row_word = []\n    for item in each_row: \n        word = cvmodel.vocabulary[item]\n        row_word.append(word)\n    results.append(row_word)\nprint(results)"}, {"cell_type": "code", "execution_count": 251, "metadata": {}, "outputs": [], "source": "test_result = {'test': results, 'weight':term_weight.collect()}\n\nwith open ('lda_model_1211.json', 'w') as f:\n       json.dump(test_result , f)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 252, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Copying file://lda_model_1211.json [Content-Type=application/json]...\n/ [1 files][  3.2 KiB/  3.2 KiB]                                                \nOperation completed over 1 objects/3.2 KiB.                                      \n"}], "source": "! gsutil cp lda_model_1211.json gs://project_rong/lda_model_1211.json"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 187, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(raw_features=SparseVector(5000, {2: 1.0, 5: 1.0, 22: 1.0, 26: 1.0, 33: 1.0, 34: 1.0, 60: 2.0, 63: 2.0, 74: 1.0, 91: 1.0, 101: 1.0, 112: 1.0, 155: 1.0, 163: 1.0, 168: 1.0, 174: 1.0, 179: 1.0, 181: 1.0, 187: 1.0, 193: 1.0, 208: 1.0, 224: 2.0, 287: 1.0, 316: 1.0, 328: 1.0, 342: 1.0, 347: 1.0, 374: 2.0, 405: 1.0, 448: 1.0, 518: 1.0, 561: 1.0, 588: 1.0, 605: 1.0, 617: 1.0, 634: 1.0, 682: 1.0, 714: 1.0, 826: 1.0, 916: 1.0, 984: 2.0, 1069: 2.0, 1507: 1.0, 1555: 1.0, 1824: 1.0, 1937: 1.0, 2902: 1.0, 3084: 1.0, 3094: 1.0, 3198: 1.0, 3652: 1.0}))]"}, "execution_count": 187, "metadata": {}, "output_type": "execute_result"}], "source": "transformed.select('raw_features').take(1)\n\nStopWords = stopwords.words(\"english\")\nStopWords"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "\n\nall_keywords = set()\nfor time in time_dict.keys(): \n    for i in time_dict[time]['keywords']:\n        all_keywords.add(i[0])\nall_keywords  \n\n\nxx1 = spark.read.json(f'gs://project_rong/cancer/cjson/c_1.json', schema=schema)\ntime_1 = xx1.filter(xx1['publication_date'] > datetime(2010,1,1)).filter(xx1['publication_date'] < datetime(2011,1,1))\ntime_1.count()\n\n\ntime_dict = {}\n# xx1.select('*').orderBy('publication_date', ascending = False).show()\n\nfor i in range(2009, 2019):\n    time_period = xx1.filter(xx1['publication_date'] >= datetime(i,1,1)).filter(xx1['publication_date'] <= datetime(i+1,1,1))\n    time_dict[i] = {}\n    time_dict[i]['total_articles'] = time_period.count()\n    time_dict[i]['keywords'] = find_keywords(time_period)[:20]\n#     for each_key in find_keywords(time_period)[:11]:\n#         time_dict[i]['keywords'][each_key][0] =each_key[1]\n    time_dict[i]['journal'] = find_journal(time_period)[:20]\n    \nprint(time_dict)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# test\n# tokens = reviews.map( lambda document: document.strip().lower()).map( lambda document: re.split(\" \", document))          \\\n#     .map( lambda word: [x for x in word if x.isalpha()])           \\\n#     .map( lambda word: [x for x in word if len(x) > 3] )           \\\n#     .map( lambda word: [x for x in word if x not in StopWords])    \\\n#     .zipWithIndex()\n\n\n# sp1 = time_1.select('abstract').rdd"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.9"}}, "nbformat": 4, "nbformat_minor": 2}